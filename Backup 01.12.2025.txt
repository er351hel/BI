#imprts
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, cross_validate, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, roc_auc_score, roc_curve, auc
import warnings
warnings.filterwarnings('ignore')

print("="*80)
print("TITANIC DATASET - TRAIN/VALIDATION SPLIT UND CROSS VALIDATION ANALYSE")
print("="*80)

# ============================================================================
# 1. DATEN LADEN UND VORBEREITEN
# ============================================================================
print("\n1. DATEN LADEN UND VORBEREITEN")
print("-" * 80)

train_df = pd.read_csv('c:/HTWG/WIN7/BI/BI/train.csv')
print(f"Datensatz geladen: {train_df.shape[0]} Passagiere, {train_df.shape[1]} Features")
print(f"Überlebensrate im Datensatz: {train_df['Survived'].mean():.2%}")

# Daten vorbereiten
X = train_df[['Pclass', 'Sex', 'Age', 'Fare']].copy()
X['Sex'] = X['Sex'].map({'male': 0, 'female': 1})

# Fehlende Werte behandeln
X['Age'].fillna(X['Age'].median(), inplace=True)
X['Fare'].fillna(X['Fare'].median(), inplace=True)

y = train_df['Survived']

print(f"Features shape: {X.shape}")
print(f"Target shape: {y.shape}")
print(f"Fehlende Werte nach Imputation: {X.isnull().sum().sum()}")

# ============================================================================
# 2. SINNVOLLE DATEN-AUFTEILUNG
# ============================================================================
print("\n2. DATEN-AUFTEILUNG: TRAIN / VALIDATION / TEST")
print("-" * 80)

# Stratifizierte Aufteilung, um Klassenverteilung zu bewahren
# Schritt 1: Aufteilen in Train (70%) und Temp (30%)
X_temp, X_test, y_temp, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Schritt 2: Aufteilen der Temp in Train (62.5%) und Validation (12.5%)
X_train, X_val, y_train, y_val = train_test_split(
    X_temp, y_temp, test_size=0.2, random_state=42, stratify=y_temp
)

print(f"Train Set:      {X_train.shape[0]:4d} Samples ({X_train.shape[0]/len(X)*100:5.1f}%) | Überlebensrate: {y_train.mean():.2%}")
print(f"Validation Set: {X_val.shape[0]:4d} Samples ({X_val.shape[0]/len(X)*100:5.1f}%) | Überlebensrate: {y_val.mean():.2%}")
print(f"Test Set:       {X_test.shape[0]:4d} Samples ({X_test.shape[0]/len(X)*100:5.1f}%) | Überlebensrate: {y_test.mean():.2%}")

print("\n✓ STRATIFIZIERTE AUFTEILUNG:")
print("  - Stratifizierung bewahrt die ursprüngliche Klassenverteilung")
print("  - Train: 70% für Model Training")
print("  - Validation: 10% für Hyperparameter-Tuning")
print("  - Test: 20% für finale Evaluation (nur am Ende)")

# Normalisierung für bessere Model-Performance
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

# ============================================================================
# 3. KLASSIFIKATIONSMODELLE TRAINIEREN
# ============================================================================
print("\n3. KLASSIFIKATIONSMODELLE TRAINIEREN")
print("-" * 80)

models = {
    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, max_depth=5)
}

trained_models = {}

for name, model in models.items():
    print(f"\nTrainiere: {name}")
    model.fit(X_train_scaled, y_train)
    trained_models[name] = model
    
    # Predictions auf Validation Set
    y_pred_val = model.predict(X_val_scaled)
    val_accuracy = accuracy_score(y_val, y_pred_val)
    
    print(f"  ✓ Training abgeschlossen")
    print(f"  → Validation Accuracy: {val_accuracy:.4f}")

# ============================================================================
# 4. CROSS-VALIDATION ANALYSE
# ============================================================================
print("\n4. CROSS-VALIDATION ANALYSE")
print("-" * 80)

print("\nWarum Cross-Validation wichtig ist:")
print("  • Nutzt den gesamten Datensatz für Training UND Evaluation")
print("  • Reduziert Bias durch zufällige Datenaufteilung")
print("  • Liefert robustere Performance-Schätzungen")
print("  • Deckt Overfitting früh auf")

# Stratified K-Fold Cross-Validation (5 Folds)
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

cv_results = {}

for name, model in models.items():
    print(f"\n{name}:")
    print("  " + "=" * 70)
    
    # Cross-Validation mit mehreren Metriken
    scoring = {
        'accuracy': 'accuracy',
        'precision': 'precision',
        'recall': 'recall',
        'f1': 'f1',
        'roc_auc': 'roc_auc'
    }
    
    cv_result = cross_validate(model, X_train_scaled, y_train, cv=cv, scoring=scoring)
    cv_results[name] = cv_result
    
    # Ergebnisse für jeden Fold anzeigen
    print("  Fold-weise Ergebnisse:")
    for fold in range(cv.get_n_splits()):
        print(f"    Fold {fold+1}: Accuracy={cv_result['test_accuracy'][fold]:.4f}, "
              f"Precision={cv_result['test_precision'][fold]:.4f}, "
              f"Recall={cv_result['test_recall'][fold]:.4f}, "
              f"F1={cv_result['test_f1'][fold]:.4f}")
    
    # Zusammenfassung
    print("\n  ZUSAMMENFASSUNG DER CROSS-VALIDATION:")
    print(f"    Accuracy:  {cv_result['test_accuracy'].mean():.4f} (+/- {cv_result['test_accuracy'].std():.4f})")
    print(f"    Precision: {cv_result['test_precision'].mean():.4f} (+/- {cv_result['test_precision'].std():.4f})")
    print(f"    Recall:    {cv_result['test_recall'].mean():.4f} (+/- {cv_result['test_recall'].std():.4f})")
    print(f"    F1-Score:  {cv_result['test_f1'].mean():.4f} (+/- {cv_result['test_f1'].std():.4f})")
    print(f"    ROC-AUC:   {cv_result['test_roc_auc'].mean():.4f} (+/- {cv_result['test_roc_auc'].std():.4f})")

# ============================================================================
# 5. INTERPRETATION DER CROSS-VALIDATION ERGEBNISSE
# ============================================================================
print("\n5. INTERPRETATION DER CROSS-VALIDATION ERGEBNISSE")
print("-" * 80)

print("\nWIE MAN DIE ERGEBNISSE DEUTET:")
print("""
  MITTELWERT (z.B. Accuracy: 0.7823):
  • Das ist die durchschnittliche Performance über alle 5 Folds
  • Erwartete Genauigkeit bei neuen, unbekannten Daten
  
  STANDARDABWEICHUNG (z.B. ±0.0456):
  • Zeigt die STABILITÄT des Modells
  • Kleine Std-Dev (< 0.03): Stabiles Modell, zuverlässige Predictions
  • Große Std-Dev (> 0.05): Modell ist anfällig für spezifische Datenaufteilungen
  
  ÜBERFITTING-ERKENNUNG:
  • Wenn Std-Dev sehr groß ist: Modell passt zu stark zu Training-Daten
  • Wenn Train-Accuracy viel höher als CV-Accuracy: Warnsignal für Overfitting
  
  METRIKEN IM DETAIL:
  • ACCURACY: Allgemeiner Maßstab (Anteil korrekter Vorhersagen)
    - Problematisch bei unbalancierten Klassen
  
  • PRECISION: Von den vorhergesagten "Überlebt", wie viele wirklich überlebt?
    - Wichtig wenn False Positives kritisch sind
  
  • RECALL: Von den tatsächlichen "Überlebt", wie viele erkannt wir?
    - Wichtig wenn False Negatives kritisch sind (z.B. Krankheitserkennung)
  
  • F1-SCORE: Harmonisches Mittel von Precision und Recall
    - Beste einzelne Metrik für unbalancierte Datensätze
  
  • ROC-AUC: Fläche unter der Kurve (0 bis 1)
    - 0.5: Random Classifier
    - 1.0: Perfektes Modell
    - Robust gegen Klassenunbalance
""")

# ============================================================================
# 6. VISUELLE AUSWERTUNG
# ============================================================================
print("\n6. VISUELLE AUSWERTUNG ERSTELLEN...")
print("-" * 80)

fig = plt.figure(figsize=(16, 12))

# Plot 1: Cross-Validation Scores Vergleich
ax1 = plt.subplot(2, 3, 1)
metrics = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']
x_pos = np.arange(len(models))
width = 0.15

for i, metric in enumerate(metrics):
    means = [cv_results[name][f'test_{metric}'].mean() for name in models.keys()]
    ax1.bar(x_pos + i*width, means, width, label=metric.upper())

ax1.set_xlabel('Modelle')
ax1.set_ylabel('Score')
ax1.set_title('Cross-Validation Performance Vergleich')
ax1.set_xticks(x_pos + width * 2)
ax1.set_xticklabels(models.keys())
ax1.legend()
ax1.grid(axis='y', alpha=0.3)

# Plot 2: Accuracy mit Fehlerbalken
ax2 = plt.subplot(2, 3, 2)
accuracies_mean = [cv_results[name]['test_accuracy'].mean() for name in models.keys()]
accuracies_std = [cv_results[name]['test_accuracy'].std() for name in models.keys()]
ax2.bar(models.keys(), accuracies_mean, yerr=accuracies_std, capsize=10, alpha=0.7, color='steelblue')
ax2.set_ylabel('Accuracy')
ax2.set_title('Accuracy mit Standardabweichung\n(Fehlerbalken = Stabilität)')
ax2.set_ylim([0.7, 0.85])
ax2.grid(axis='y', alpha=0.3)

# Plot 3: Fold-weise Accuracy Verlauf
ax3 = plt.subplot(2, 3, 3)
for name in models.keys():
    fold_numbers = np.arange(1, 6)
    ax3.plot(fold_numbers, cv_results[name]['test_accuracy'], marker='o', label=name, linewidth=2)
ax3.set_xlabel('Fold Nummer')
ax3.set_ylabel('Accuracy')
ax3.set_title('Accuracy pro Fold\n(Zeigt Konsistenz über Folds)')
ax3.legend()
ax3.grid(alpha=0.3)
ax3.set_xticks([1, 2, 3, 4, 5])

# Plot 4: Confusion Matrix für bestes Modell
ax4 = plt.subplot(2, 3, 4)
best_model = trained_models['Random Forest']
y_pred_val = best_model.predict(X_val_scaled)
cm = confusion_matrix(y_val, y_pred_val)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax4, cbar=False)
ax4.set_title('Confusion Matrix (Validation Set)\nRandom Forest')
ax4.set_ylabel('Tatsächlich')
ax4.set_xlabel('Vorhergesagt')

# Plot 5: F1-Score Vergleich
ax5 = plt.subplot(2, 3, 5)
f1_means = [cv_results[name]['test_f1'].mean() for name in models.keys()]
f1_stds = [cv_results[name]['test_f1'].std() for name in models.keys()]
ax5.bar(models.keys(), f1_means, yerr=f1_stds, capsize=10, alpha=0.7, color='coral')
ax5.set_ylabel('F1-Score')
ax5.set_title('F1-Score (Beste Metrik für unbalancierte Daten)')
ax5.set_ylim([0.65, 0.80])
ax5.grid(axis='y', alpha=0.3)

# Plot 6: ROC-AUC Vergleich
ax6 = plt.subplot(2, 3, 6)
roc_means = [cv_results[name]['test_roc_auc'].mean() for name in models.keys()]
roc_stds = [cv_results[name]['test_roc_auc'].std() for name in models.keys()]
ax6.bar(models.keys(), roc_means, yerr=roc_stds, capsize=10, alpha=0.7, color='lightgreen')
ax6.set_ylabel('ROC-AUC')
ax6.set_title('ROC-AUC Score (Robust gegen Klassenunbalance)')
ax6.set_ylim([0.7, 0.85])
ax6.grid(axis='y', alpha=0.3)

plt.tight_layout()
plt.savefig('c:/HTWG/WIN7/BI/BI/cross_validation_analysis.png', dpi=300, bbox_inches='tight')
print("\n✓ Grafik gespeichert: cross_validation_analysis.png")
plt.show()

# ============================================================================
# 6b. S-KURVE (SIGMOID) FÜR LOGISTISCHE REGRESSION (EIN FEATURE)
# ============================================================================
def plot_logistic_s_curve(feature_name='Age'):
  """
  Trainiert eine einfache logistische Regression nur auf `feature_name`
  und plottet die vorhergesagte Wahrscheinlichkeit (S-Kurve) über den
  Wertebereich des Features.
  """
  # Bereite Feature vor
  Xf = X_train[[feature_name]].copy()
  Xf[feature_name] = Xf[feature_name].fillna(Xf[feature_name].median())
  yf = y_train.copy()

  # Skalieren des einzelnen Features
  scaler_f = StandardScaler()
  Xf_scaled = scaler_f.fit_transform(Xf)

  # Trainiere ein einfaches LogReg-Modell nur auf dem Feature
  lr = LogisticRegression(random_state=42, max_iter=1000)
  lr.fit(Xf_scaled, yf)

  # Gitter im Original-Feature-Raum zum Plotten
  x_min, x_max = Xf[feature_name].min(), Xf[feature_name].max()
  x_vals = np.linspace(x_min, x_max, 300).reshape(-1, 1)
  x_vals_scaled = scaler_f.transform(x_vals)

  probs = lr.predict_proba(x_vals_scaled)[:, 1]

  plt.figure(figsize=(8, 6))
  plt.plot(x_vals, probs, color='darkorange', lw=2, label='Predicted P(y=1)')

  # Scatter der Trainingsdaten (mit leichtem Jitter für Sichtbarkeit)
  jitter = np.random.normal(0, 0.02, size=len(yf))
  plt.scatter(Xf[feature_name], yf + jitter, alpha=0.35, s=20, label='Train data (jitter)')

  # Entscheidungsschwelle und ungefähre Entscheidungsgrenze im Feature-Raum
  plt.axhline(0.5, color='gray', linestyle='--', label='Threshold = 0.5')
  try:
    idx = np.argmin(np.abs(probs - 0.5))
    x_decision = x_vals[idx, 0]
    plt.axvline(x_decision, color='gray', linestyle=':', label=f'Boundary ≈ {x_decision:.2f}')
  except Exception:
    pass

  plt.xlabel(feature_name)
  plt.ylabel('Predicted probability')
  plt.title(f'Logistic Regression S-Kurve für Feature: {feature_name}')
  plt.legend()
  plt.grid(alpha=0.25)

  save_file = f'c:/HTWG/WIN7/BI/BI/logistic_s_curve_{feature_name}.png'
  plt.tight_layout()
  plt.savefig(save_file, dpi=300, bbox_inches='tight')
  print(f"✓ S-Kurve gespeichert: {save_file}")
  plt.show()


# Erstelle S-Kurven für typische kontinuierliche Features
for feature in ['Age', 'Fare']:
  if feature in X_train.columns:
    print(f"\nErstelle S-Kurve für Feature: {feature}")
    plot_logistic_s_curve(feature)

# ============================================================================
# 7. DETAILLIERTE KLASSIFIKATIONS-BERICHTE
# ============================================================================
print("\n7. DETAILLIERTE KLASSIFIKATIONSBERICHTE")
print("-" * 80)

for name, model in trained_models.items():
    print(f"\n{name}:")
    print("=" * 70)
    y_pred = model.predict(X_val_scaled)
    print(classification_report(y_val, y_pred, target_names=['Nicht überlebt', 'Überlebt']))

print("\n" + "="*80)
print("ANALYSE ABGESCHLOSSEN")
print("="*80)
